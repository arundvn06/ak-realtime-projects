{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4bYiHDNDD8Mb"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"code","metadata":{"id":"Alui-GbzEBtW","cellView":"form","executionInfo":{"status":"ok","timestamp":1717236911370,"user_tz":-330,"elapsed":599,"user":{"displayName":"AIML Support","userId":"10944637975474083227"}},"outputId":"ad8b1ff8-02f2-49e9-8c07-99a64d2aabc1","colab":{"base_uri":"https://localhost:8080/","height":502}},"source":["#@title Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3a_face_recognition.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3a_face_recognition.mp4\" type=\"video/mp4\">\n","</video>\n"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"5pKTV6SeF3Fs"},"source":["Face recognition is a method of identifying or verifying the identity of an individual using their face. Face recognition systems can be used to identify people in photos, video, or in real-time. The recent major breakthrough in increasing the face recognition accuracy has come from advancements in Deep Learning and CNNs."]},{"cell_type":"markdown","metadata":{"id":"YL0x5EO6Bzqb"},"source":["**Objectives:**\n","\n","**Stage 1:** Build a Siamese Network and obtain the Siamese Representation for the AT&T faces dataset\n","\n","**Stage 2 (10 Marks):** Use the same Siamese Model to test for Face Similarity on the Mobile App\n","\n","**Stage 3 (10 Marks):** Get the Siamese Network Representation of the Team Data and build a classifier to perform Face Recognition on the Mobile app"]},{"cell_type":"markdown","metadata":{"id":"zgzluE32QbbZ"},"source":["\n","##**Stage 1: Build a Siamese Network**\n","\n","\n","---\n","\n","\n","* Define a Siamese network and obtain Siamese Representation on the AT&T Faces Dataset (code given)\n"]},{"cell_type":"markdown","metadata":{"id":"xr4GhV_1EHw7"},"source":["**Dataset download**"]},{"cell_type":"markdown","metadata":{"id":"6mIZg5TkgLXE"},"source":["The [AT&T Faces Dataset](https://git-disl.github.io/GTDLBench/figures/faces.gif) contains 10 different images of each of 40 distinct persons. Images were taken with variations in times, lighting, facial expressions and facial details (eg. glasses / no glasses).\n","\n","Dataset Statistics: Color: Grayscale; Sample Size: 92x112; Total Samples: 400; Dataset Size: 4.5 MB (compressed in .tar.z)"]},{"cell_type":"code","metadata":{"id":"YmH6661pEJtx","cellView":"form"},"source":["#@title #####Download the AT&T Dataset.\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","\n","notebook=\"M3_Hackathon\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/FaceRecogHackathon/Datasets/ATandT/data-20190607T005435Z-001.zip\")\n","    ipython.magic(\"sx unzip -qq data-20190607T005435Z-001.zip\")\n","\n","    ipython.magic(\"sx pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.2.1\")\n","    ipython.magic(\"sx pip install opencv-python\")\n","    ipython.magic(\"sx pip uninstall -y scikit-learn\")\n","    ipython.magic(\"sx pip install scikit-learn==0.22.2.post1\")\n","    print (\"Setup completed successfully\")\n","    return\n","setup()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_88vzTEOmPt"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WtJ-FReH6rVL"},"source":["#### **Imports: All the imports are defined here**"]},{"cell_type":"code","source":["!pip install -U scikit-learn"],"metadata":{"id":"4CGvjmePBHue"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFDRRZz86rVM"},"source":["%matplotlib inline\n","import torchvision\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","import matplotlib.pyplot as plt\n","import torchvision.utils\n","import numpy as np\n","import random\n","from PIL import Image\n","import torch\n","from torch.autograd import Variable\n","import PIL.ImageOps\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqaeLqVz6rVR"},"source":["**Helper functions**"]},{"cell_type":"code","metadata":{"id":"YYV5ncY46rVS"},"source":["## The below function plots a given tensor image\n","def imshow(img,text=None,should_save=False):\n","    npimg = img.numpy()\n","    plt.axis(\"off\")\n","    if text:\n","        plt.text(75, 8, text, style='italic',fontweight='bold', bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","    # In PyTorch, the order of dimension is \"channel*width*height\" but in matplotlib it is \"width*height*channel\".\n","    # So a transpose is performed to correctly index the dimensions\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","## We will use the below method to plot the loss graph while training\n","def show_plot(iteration,loss):\n","    plt.plot(iteration,loss)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcfEoWKOa9kw"},"source":["**Configuration class**"]},{"cell_type":"code","metadata":{"id":"5Aa9M6KP6rVW"},"source":["# A simple class to manage all configurations\n","class Config():\n","    training_dir = \"./data/faces/training/\"\n","    testing_dir = \"./data/faces/testing/\"\n","    train_batch_size = 64\n","    train_number_epochs = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6cQAOjOi46e"},"source":["**Display a sample image**"]},{"cell_type":"code","metadata":{"id":"F1q_CfdUJtGk"},"source":["import cv2\n","from google.colab.patches import cv2_imshow\n","\n","# Opencv's imread takes full path of an image as input to read an image\n","im1 = cv2.imread('./data/faces/training/s1/10.pgm')\n","print(im1.shape)\n","cv2_imshow(im1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyrVxVpi6rVZ"},"source":["#### **Define a custom SiameseNetworkDataset class**\n","\n","This dataset generates a pair of images. 0 for geniune pair (similar) and 1 for imposter pair (dissimilar)"]},{"cell_type":"code","metadata":{"id":"crHPscPS6rVa"},"source":["# Below is the class of custom dataset which arranges images in pairs and gives the labels. if both are same, then label will be 0 otherwise 1\n","# As our images are in .pgm extension (portable gray map) we converting them 'L' to store luminance which is basically single channel image.\n","\n","class SiameseNetworkDataset(Dataset):\n","    def __init__(self,imageFolderDataset, transform=None):\n","        self.imageFolderDataset = imageFolderDataset\n","        self.transform = transform\n","\n","    # Overriding the data retriever (__getitem__) to provide a pair of images + similar/dissimilar label\n","    def __getitem__(self,index):\n","        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n","        #we need to make sure approx 50% of images are in the same class\n","        should_get_same_class = random.randint(0,1)\n","        if should_get_same_class:\n","            while True:\n","                #keep looping till the same class image is found\n","                img1_tuple = random.choice(self.imageFolderDataset.imgs)\n","                if img0_tuple[1]==img1_tuple[1]:\n","                    break\n","        else:\n","            while True:\n","                #keep looping till a different class image is found\n","                img1_tuple = random.choice(self.imageFolderDataset.imgs)\n","                if img0_tuple[1] !=img1_tuple[1]:\n","                    break\n","\n","        img0 = Image.open(img0_tuple[0])\n","        img1 = Image.open(img1_tuple[0])\n","        img0 = img0.convert(\"L\")\n","        img1 = img1.convert(\"L\")\n","\n","        if self.transform is not None:\n","            img0 = self.transform(img0)\n","            img1 = self.transform(img1)\n","\n","        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n","\n","    def __len__(self):\n","        return len(self.imageFolderDataset.imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rw07TOrE6rVe"},"source":["**Create an Image Folder Dataset**"]},{"cell_type":"code","metadata":{"id":"WcbAyg5i6rVf"},"source":["folder_dataset = dset.ImageFolder(root=Config.training_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhTk0Ez_cP-x"},"source":["**Create the Siamese Network Dataset**"]},{"cell_type":"code","metadata":{"id":"iQsKRw0x6rVi"},"source":["# Create the object for the SiameseNetworkDataset class (defined earlier in this notebook);\n","\n","# NOTE: the 'TRANSFORMS' HERE CONSISTS OF\n","# a) resizing to 100*100\n","# b) Converting to tensor.\n","\n","# YOU HAVE TO APPLY THE SAME TRANSORMS WHEN DEPLOYING THE MODEL ON THE SERVER!!\n","\n","siamese_dataset = SiameseNetworkDataset(imageFolderDataset = folder_dataset, transform = transforms.Compose([transforms.Resize((100,100)), transforms.ToTensor()]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ewiz1R8n6rVl"},"source":["**Visualising some of the data**\n","The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n","1 indiciates dissimilar, and 0 indicates similar."]},{"cell_type":"code","metadata":{"id":"ba3Q53wB6rVm"},"source":["vis_dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=8, batch_size=8)\n","\n","dataiter = iter(vis_dataloader)\n","\n","example_batch = next(dataiter)\n","concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n","imshow(torchvision.utils.make_grid(concatenated))\n","print(example_batch[2].numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"LMIKoOJw6rVu"},"source":["#### **Neural Net Definition**\n","\n","We will use a standard convolutional neural network"]},{"cell_type":"code","metadata":{"id":"Q1KKqJXv6rVv"},"source":["class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","        self.cnn1 = nn.Sequential(\n","            nn.ReflectionPad2d(1),       #Pads the input tensor using the reflection of the input boundary, it similar to the padding.\n","            nn.Conv2d(1, 4, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            nn.BatchNorm2d(4),\n","\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(4, 8, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            nn.BatchNorm2d(8),\n","\n","\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(8, 8, kernel_size=3),\n","            nn.ReLU(inplace=True),\n","            nn.BatchNorm2d(8),\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(8*100*100, 500),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(500, 500),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(500, 5))\n","\n","    # forward_once is for one image. This can be used while classifying the face images\n","    def forward_once(self, x):\n","        output = self.cnn1(x)\n","        output = output.view(output.size()[0], -1)\n","        output = self.fc1(output)\n","        return output\n","\n","    def forward(self, input1, input2):\n","        output1 = self.forward_once(input1)\n","        output2 = self.forward_once(input2)\n","        return output1, output2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fApzle9M6rVy"},"source":["**Contrastive Loss**"]},{"cell_type":"code","metadata":{"id":"4lf8WOsh6rVz"},"source":["class ContrastiveLoss(torch.nn.Module):\n","    \"\"\"\n","    Contrastive loss function.\n","    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n","    \"\"\"\n","\n","    def __init__(self, margin=2.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, output1, output2, label):\n","        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n","        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n","                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n","\n","        return loss_contrastive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xodTMbk36rV2"},"source":["**Training the model**"]},{"cell_type":"code","metadata":{"id":"kmbZFlNM6rV3"},"source":["train_dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=8, batch_size=Config.train_batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0u4FAXo6rV6"},"source":["net = SiameseNetwork().cuda()\n","criterion = ContrastiveLoss()\n","optimizer = optim.Adam(net.parameters(),lr = 0.0005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xB8b63Tb6rV-"},"source":["counter = []\n","loss_history = []\n","iteration_number= 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"FMMyf_4R6rWC"},"source":["%%capture\n","for epoch in range(0,Config.train_number_epochs):\n","    for i, data in enumerate(train_dataloader,0):\n","        img0, img1 , label = data\n","        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n","        optimizer.zero_grad()\n","        output1,output2 = net(img0,img1)\n","        loss_contrastive = criterion(output1,output2,label)\n","        loss_contrastive.backward()\n","        optimizer.step()\n","        if i %10 == 0 :\n","            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n","            iteration_number +=10\n","            counter.append(iteration_number)\n","            loss_history.append(loss_contrastive.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_Nujg6NVAzN"},"source":["# Plotting the loss graph using the function show_plot\n","show_plot(counter,loss_history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HK86aP_mrOcK"},"source":["**Saving and loading model**\n","\n","https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","metadata":{"id":"jDD7UHYm8fDe"},"source":["## Saving the model as a state dictionary\n","state = {'net_dict': net.state_dict()}\n","torch.save(state, './siamese_model.t7')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qs0umEbPW8c"},"source":["**Verifying the saved model with load_state_dict**"]},{"cell_type":"code","metadata":{"id":"UVtGQY1MMEVk"},"source":["myModel = SiameseNetwork().cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2Vqzxl6ppZR"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EgLGRkEMWzx"},"source":["ckpt = torch.load('./siamese_model.t7')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUohW3VLrqU8"},"source":["# Save the state dictionary of the Siamese network (use pytorch only), It will be useful in integrating to the mobile app\n","# A state_dict is simply a Python dictionary object that maps each layer of the network to its parameters (weights)\n","# As a Python dictionary it can be easily saved, updated, altered and restored, adding a great deal of modularity to PyTorch models\n","\n","myModel.load_state_dict(ckpt['net_dict'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1qjUdd_-6rWG"},"source":["##### Create the siamese dataset for the `testing_dir` and also the respective data loader. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found the images more similar, while higher values indicate it found them to be dissimilar."]},{"cell_type":"code","metadata":{"run_control":{"marked":true},"scrolled":false,"id":"S52pI-9B6rWH"},"source":["folder_dataset_test = dset.ImageFolder(root = Config.testing_dir) #testing_dir\n","siamese_dataset = SiameseNetworkDataset(imageFolderDataset = folder_dataset_test,\n","                                        transform = transforms.Compose([transforms.Resize((100,100)), transforms.ToTensor()]))\n","\n","siamese_dataset = nc.SafeDataset(siamese_dataset)\n","\n","test_dataloader = DataLoader(siamese_dataset,num_workers=6,batch_size=1,shuffle=True)\n","dataiter = iter(test_dataloader)\n","x0,_,_ = next(dataiter)\n","\n","# Check the similarity for the first few images from the test data loader\n","for i in range(10):\n","    _,x1,label2 = next(dataiter)\n","    concatenated = torch.cat((x0,x1),0)\n","\n","    output1,output2 = myModel(Variable(x0).cuda(),Variable(x1).cuda()) #using the loaded 'myModel'; even 'net' can be used\n","    #..but using 'myModel' helps a quick test on if the save and load model is working fine. You will replicate the same\n","    #..in the server.\n","\n","    euclidean_distance = F.pairwise_distance(output1, output2)\n","    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.item()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qyky_fUnljGn"},"source":["##**Stage 2: Test for Face Similarity on Mobile App (10 Marks)**\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8zjScEEZOycf"},"source":["1. Server login and activation steps:\n","\n","    - A username and password will be provided to your team on the day of the Hackathon.\n","\n","    - Open the terminal (Command Prompt)\n","  \n","  - Login to SSH by typing ssh (username)@aiml-sandbox1.talentsprint.com. Give the login username which is given to you. Type in the password in the next step. Note that the cursor does not move while typing the password.\n","\n","   `Eg: ssh b16h3gxx@aiml-sandbox1.talentsprint.com`\n","\n","    (If it is your first time connecting to the server from this computer, accept the connection by typing \"yes\".)\n","    - After logging into SSH, activate your virtual environment using the\n","command **source venv/bin/activate** and then press enter\n","    - You can start the server by giving the command **sh runserver.sh** and then press enter.\n","\n","2. Download the Siamese model and upload it in the ftp server (refer to [Filezilla Installation and Configuration document](https://drive.google.com/file/d/1hnMXcwpCwAz94ljAhtsJdQSfe-JaOD5K/view?usp=sharing))\n","\n","\n","3. Update the Siamese model architecture in the **face_recognition_model.py** file and provide the code in the **'get_similarity()'** function of the **face_recognition.py file**. (See Deployment related files)\n","\n","4. Test the Siamese model in the Mobile app using its Face Similarity application\n","\n","  - Capture your pic twice and it will return a number between 0 (similar) and 1 (dissimilar) indicating the similarity measure"]},{"cell_type":"markdown","metadata":{"id":"725KzSwcDlq8"},"source":["##**Stage 3: Build Face Recognition Classification Model with Siamese Representation of Team Data and Test on Mobile App (10 Marks)**\n","\n","\n","---\n","\n","\n","- Build a Face Recognition Classification Model on Team data\n","    - Collect Team data on the EFR app (activate the server first)\n","    \n","    - Follow the \"Mobile_APP_Documentation\" to collect the Faces of your team. These will be stored in the server, for which the login is provided to you. [Mobile_APP_Documentation](https://drive.google.com/file/d/1tpr8_U0Ll_TexN4s-0pmPPg23J7Usok2/view?usp=sharing)\n","\n","    - Train a classifier (use any classifier) with the features extracted from the above trained Siamese network, for your Team Data\n","    - Save the Classification Model with joblib (if Sklearn classification model is used)\n","\n","- Deploy on the Server\n","\n","    - Download the trained models (Siamese model, Classification model) and upload them in the ftp server (refer to [Filezilla Installation and Configuration document](https://drive.google.com/file/d/1hnMXcwpCwAz94ljAhtsJdQSfe-JaOD5K/view?usp=sharing))\n","    - Update the Siamese model architecture in the face_recognition_model.py file (If Required) and the code in the **'get_face_class()'** function of the face_recognition.py file. (See Deployment related files)\n","\n","- Test the model using the Face Recognition application in the EFR Mobile App"]},{"cell_type":"markdown","metadata":{"id":"m2OmDtmQPHvy"},"source":["\n","**Download your team data from the EFR app into your colab notebook using the links provided below.**\n","\n","NOTE: Replace the string \"username\" with your login username (such as b16h3gxx) in the cell below for face images.\n","\n","This data will be useful while training the networks."]},{"cell_type":"code","metadata":{"id":"D34fUmkfPQBO"},"source":["!wget -nH --recursive --no-parent --reject 'index.*' https://aiml-sandbox.talentsprint.com/expression_detection/username/captured_face_images/ --cut-dirs=3  -P ./captured_face_images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO6_8jRpgyBr"},"source":["# Here simple ImageFolder is enough; we don't need SiameseDataSet\n","finalClassifierDset = dset.ImageFolder(root='./captured_face_images',\n","                                       transform = transforms.Compose([transforms.Grayscale(num_output_channels = 1), transforms.Resize((100,100)), transforms.ToTensor()]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L188txY5j8Ty"},"source":["# Dataloader using the dataset created above.\n","representation_dataloader = DataLoader(finalClassifierDset, shuffle=False, num_workers=8, batch_size=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Nb1QVhWP3Kv"},"source":["# Load the state dict of the siamese model\n","# <YOUR CODE HERE>\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct4gtqR6P6xu"},"source":["# Get a siamese representation of each of your data points i.e. for each of your team images.\n","## For example (if your image is of the size 100*100 above)\n","# <YOUR CODE HERE>\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p0lUxkaEkaxE"},"source":["**Train a Classifier for Face Recognition**\n","\n","You can use any classifier with the features extracted from the above trained Siamese network of your team data. If required, you have to convert torch variable to numpy array before using SkLearn."]},{"cell_type":"code","metadata":{"id":"k8M9OpEOP9Sg"},"source":["# YOUR CODE HERE for training a classifier. You can use simple MLP or Sklearn models.\n","# Note: Ensure you convert torch variable to numpy array before using SkLearn.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1EBMNS5QDi7"},"source":["**Save your classification model**\n","\n","* Save your sklearn models using joblib\n","\n"]},{"cell_type":"code","metadata":{"id":"RdQtj8RgQD6D"},"source":["# YOUR CODE HERE for saving the model.\n","# Note: For SkLearn classifier use joblib for saving the model.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYNNdQ9NQIxj"},"source":["**Download your trained model using the code below**\n","* Given the path of model file the following code downloads it through the browser"]},{"cell_type":"code","metadata":{"id":"g_i80BtsQGfB"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]}]}